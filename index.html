<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Huichi Zhou (Âë®ËæâÊ±†) - AI Researcher</title>

  <meta name="author" content="Huichi Zhou">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="description" content="Huichi Zhou - PhD student at UCL researching Adversarial Machine Learning and Large Language Models">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üöÄ</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Huichi Zhou</name>
              </p>
              <p>
				I am a PhD student at University College London (UCL), advised by Professor <a href="https://scholar.google.com/citations?hl=en&user=wIE1tY4AAAAJ&view_op=list_works" target="_blank">Jun Wang</a>.
				Previously, I was a graduate student at <a href="https://www.imperial.ac.uk/">Imperial College London</a> with
                YangLab advised by Professor <a href="https://scholar.google.com.mx/citations?hl=zh-CN&user=ZfzEFpsAAAAJ&view_op=list_works">Guang Yang</a>.
              </p>
              <p>
                My research interest is broadly in Adversarial Machine Learning and Large Language Models.
	      </p>
              <p>
                üìö <strong><span id="citation-count-inline">150+</span></strong> citations | üìù <strong>14</strong> publications
              </p>
              <p>
                Email: h.zhou24 [at] imperial.ac.uk
              </p>
	      <p style="text-align:center;">
                <a href="https://github.com/HuichiZhou" target="_blank">üêô Github</a>
                <a href="https://scholar.google.com/citations?user=1IJyxpUAAAAJ&hl=en" target="_blank">üéì Google Scholar</a>
              </p>


            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/zhc.jpg"><img style="width:80%;max-width:100%" alt="profile photo" src="images/zhc.jpg" class="hoverZoomLink"></a>
            </td>
		  
          </tr>
        </tbody></table>

        <!-- Publications Section -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Publications</heading>
              <p style="margin-top:10px;color:#718096;font-size:14px;">* denotes equal contribution</p>
            </td>
          </tr>
        </tbody></table>

        <!-- Research Categories -->
        <div style="max-width:800px;margin:0 auto;">

          <!-- Category: Trustworthy AI & LLMs -->
          <div class="category-section">
            <div class="category-header" onclick="toggleCategory('llm')">
              <span class="category-icon">ü§ñ</span>
              <span class="category-title">Trustworthy AI & Large Language Models</span>
              <span class="category-count">7 papers</span>
              <span class="toggle-icon" id="llm-toggle">‚ñº</span>
            </div>
            <div class="category-content" id="llm-content">

              <!-- Memento -->
              <div class="paper-card" data-github="Agent-on-the-Fly/Memento">
                <div class="paper-badge preprint">Preprint</div>
                <papertitle>Memento: Fine-tuning LLM Agents without Fine-tuning LLMs</papertitle>
                <div class="paper-authors"><strong>Huichi Zhou</strong>, Yihang Chen, Siyuan Guo, Xue Yan, Kin Hei Lee, Zihan Wang, Ka Yiu Lee, Guchun Zhang, Kun Shao, Linyi Yang, Jun Wang</div>
                <div class="paper-venue">arXiv 2025/8</div>
                <div class="paper-links">
                  <a href="#" target="_blank">üìÑ Paper</a>
                  <a href="https://github.com/Agent-on-the-Fly/Memento" target="_blank" class="github-link">üíª Code <span class="github-stars" data-repo="Agent-on-the-Fly/Memento">‚≠ê --</span></a>
                </div>
              </div>

              <!-- TrustRAG AAAI Workshop -->
              <div class="paper-card" data-github="HuichiZhou/TrustRAG">
                <div class="paper-badge workshop">Workshop</div>
                <papertitle>TrustRAG: Enhancing Robustness and Trustworthiness in RAG</papertitle>
                <div class="paper-authors"><strong>Huichi Zhou*</strong>, Kin-Hei Lee*, Zhonghao Zhan*, Yue Chen, Zhenhao Li, Zhaoyang Wang, Hamed Haddadi, Emine Yilmaz</div>
                <div class="paper-venue">AAAI 2026 Workshop on Trust and Control in Agentic AI (TrustAgent)</div>
                <div class="paper-links">
                  <a href="https://trust-rag.github.io/" target="_blank">üåê Project</a>
                  <a href="https://arxiv.org/pdf/2501.00879" target="_blank">üìÑ Paper</a>
                  <a href="https://github.com/HuichiZhou/TrustRAG" target="_blank" class="github-link">üíª Code <span class="github-stars" data-repo="HuichiZhou/TrustRAG">‚≠ê --</span></a>
                </div>
              </div>

              <!-- GEMA-Score -->
              <div class="paper-card">
                <div class="paper-badge conference">Conference</div>
                <papertitle>GEMA-Score: Granular Explainable Multi-Agent Score for Radiology Report Evaluation</papertitle>
                <div class="paper-authors">Zhongjian Zhang, Kin-Hei Lee, Wanqing Deng, <strong>Huichi Zhou</strong>, Zhifan Jin, Jingyu Huang, Zhifan Gao, David C Marshall, et al.</div>
                <div class="paper-venue">AAAI 2026 - The Fortieth AAAI Conference on Artificial Intelligence</div>
                <div class="paper-links">
                  <a href="#" target="_blank">üìÑ Paper</a>
                </div>
              </div>

              <!-- Reliable Foundation Models -->
              <div class="paper-card">
                <div class="paper-badge journal">Journal</div>
                <papertitle>Reliable and Responsible Foundation Models</papertitle>
                <div class="paper-authors">Xinyu Yang, Jinghan Han, et al.</div>
                <div class="paper-venue">TMLR 2025 - Transactions on Machine Learning Research</div>
                <div class="paper-links">
                  <a href="#" target="_blank">üìÑ Paper</a>
                </div>
              </div>

              <!-- DiffuseDef -->
              <div class="paper-card">
                <div class="paper-badge conference">Conference</div>
                <papertitle>DiffuseDef: Improve Robustness to Adversarial Attacks via Iterative Denoising</papertitle>
                <div class="paper-authors">Zhenhao Li, <strong>Huichi Zhou</strong>, Marek Rei, Lucia Specia</div>
                <div class="paper-venue">ACL 2025</div>
                <div class="paper-links">
                  <a href="https://arxiv.org/abs/2407.00248" target="_blank">üìÑ Paper</a>
                </div>
              </div>

              <!-- Verifiable Format Control -->
              <div class="paper-card" data-github="jinqij/VFF">
                <div class="paper-badge conference">Conference</div>
                <papertitle>Verifiable Format Control for Large Language Model Generations</papertitle>
                <div class="paper-authors">Zhaoyang Wang*, Jinqi Jiang*, <strong>Huichi Zhou*</strong>, Wenhao Zheng, Xuchao Zhang, Chetan Bansal, Huaxiu Yao</div>
                <div class="paper-venue">NAACL 2025 Findings</div>
                <div class="paper-links">
                  <a href="https://arxiv.org/pdf/2502.04498" target="_blank">üìÑ Paper</a>
                  <a href="https://huggingface.co/datasets/jinqij/VFF" target="_blank">üíª Code</a>
                </div>
              </div>

              <!-- AVLLM -->
              <div class="paper-card" data-github="HuichiZhou/AVLLM">
                <div class="paper-badge conference">Conference</div>
                <papertitle>Evaluating the Validity of Word-level Adversarial Attacks with Large Language Models</papertitle>
                <div class="paper-authors"><strong>Huichi Zhou*</strong>, Zhaoyang Wang*, Hongtao Wang, Dongping Chen, Wenhan Mu, Fangyuan Zhang</div>
                <div class="paper-venue">ACL 2024 Findings</div>
                <div class="paper-links">
                  <a href="https://aclanthology.org/2024.findings-acl.292" target="_blank">üìÑ Paper</a>
                  <a href="https://github.com/HuichiZhou/AVLLM" target="_blank" class="github-link">üíª Code <span class="github-stars" data-repo="HuichiZhou/AVLLM">‚≠ê --</span></a>
                </div>
              </div>

            </div>
          </div>

          <!-- Category: Graph Neural Networks -->
          <div class="category-section">
            <div class="category-header" onclick="toggleCategory('gnn')">
              <span class="category-icon">üï∏Ô∏è</span>
              <span class="category-title">Graph Neural Networks & Adversarial Robustness</span>
              <span class="category-count">2 papers</span>
              <span class="toggle-icon" id="gnn-toggle">‚ñº</span>
            </div>
            <div class="category-content" id="gnn-content">

              <!-- GNN-NIDS -->
              <div class="paper-card">
                <div class="paper-badge workshop">Poster</div>
                <papertitle>Enhancing GNN Robustness for Network Intrusion Detection via Agent-based Analysis</papertitle>
                <div class="paper-authors">Zhonghao Zhan, <strong>Huichi Zhou</strong>, Hamed Haddadi</div>
                <div class="paper-venue">EuroS&P 2025 Poster</div>
                <div class="paper-links">
                  <a href="https://huichizhou.github.io/" target="_blank">üìÑ Paper</a>
                </div>
              </div>

              <!-- LLM4RGNN -->
              <div class="paper-card" data-github="zhongjian-zhang/LLM4RGNN">
                <div class="paper-badge conference">Conference</div>
                <papertitle>Can Large Language Models Improve the Adversarial Robustness of Graph Neural Networks?</papertitle>
                <div class="paper-authors">Zhongjian Zhang*, Xiao Wang*, <strong>Huichi Zhou</strong>, Mengmei Zhang, Cheng Yang, Chuan Shi</div>
                <div class="paper-venue">KDD 2025 Research Track</div>
                <div class="paper-links">
                  <a href="https://arxiv.org/pdf/2408.08685" target="_blank">üìÑ Paper</a>
                  <a href="https://github.com/zhongjian-zhang/LLM4RGNN" target="_blank" class="github-link">üíª Code <span class="github-stars" data-repo="zhongjian-zhang/LLM4RGNN">‚≠ê --</span></a>
                </div>
              </div>

            </div>
          </div>

          <!-- Category: Medical AI -->
          <div class="category-section">
            <div class="category-header" onclick="toggleCategory('medical')">
              <span class="category-icon">üè•</span>
              <span class="category-title">Medical AI & Vision-Language Models</span>
              <span class="category-count">2 papers</span>
              <span class="toggle-icon" id="medical-toggle">‚ñº</span>
            </div>
            <div class="category-content" id="medical-content">

              <!-- Beyond the Hype -->
              <div class="paper-card">
                <div class="paper-badge journal">Journal</div>
                <papertitle>Beyond the Hype: A Dispassionate Look at Vision-Language Models in Medical Scenario</papertitle>
                <div class="paper-authors">Yang Nan*, <strong>Huichi Zhou*</strong>, Xiaodan Xing, Guang Yang</div>
                <div class="paper-venue">IEEE TNNLS (Impact Factor: 10.2)</div>
                <div class="paper-links">
                  <a href="https://arxiv.org/abs/2408.08704" target="_blank">üìÑ Paper</a>
                </div>
              </div>

              <!-- Medical Image Retrieval -->
              <div class="paper-card">
                <div class="paper-badge journal">Journal</div>
                <papertitle>Revisiting Medical Image Retrieval via Knowledge Consolidation</papertitle>
                <div class="paper-authors">Yang Nan, <strong>Huichi Zhou</strong>, Xiaodan Xing, Giorgos Papanastasiou, Lei Zhu, Zhifan Gao, Alejandro F. Frangi, Guang Yang</div>
                <div class="paper-venue">Medical Image Analysis (Impact Factor: 10.7)</div>
                <div class="paper-links">
                  <a href="https://arxiv.org/pdf/2503.09370" target="_blank">üìÑ Paper</a>
                </div>
              </div>

            </div>
          </div>

          <!-- Category: Multimodal AI -->
          <div class="category-section">
            <div class="category-header" onclick="toggleCategory('multimodal')">
              <span class="category-icon">üé®</span>
              <span class="category-title">Multimodal AI & Evaluation</span>
              <span class="category-count">2 papers</span>
              <span class="toggle-icon" id="multimodal-toggle">‚ñº</span>
            </div>
            <div class="category-content" id="multimodal-content">

              <!-- GUI-World -->
              <div class="paper-card" data-github="Dongping-Chen/GUI-World">
                <div class="paper-badge conference">Conference</div>
                <papertitle>GUI-World: A Video Benchmark and Dataset for Multimodal GUI-oriented Understanding</papertitle>
                <div class="paper-authors">Dongping Chen, Yue Huang, Siyuan Wu, Jingyu Tang, <strong>Huichi Zhou</strong>, Qihui Zhang, Zhigang He, Yilin Bai, Chujie Gao, Liuyi Chen, Yiqiang Li, Chenlong Wang, Yue Yu, Tianshuo Zhou, Zhen Li, Yi Gui, Yao Wan, Pan Zhou, Jianfeng Gao, Lichao Sun</div>
                <div class="paper-venue">ICLR 2025</div>
                <div class="paper-links">
                  <a href="https://openreview.net/forum?id=QarKTT5brZ" target="_blank">üìÑ Paper</a>
                  <a href="https://github.com/Dongping-Chen/GUI-World" target="_blank" class="github-link">üíª Code <span class="github-stars" data-repo="Dongping-Chen/GUI-World">‚≠ê --</span></a>
                </div>
              </div>

              <!-- MLLM-as-a-Judge -->
              <div class="paper-card" data-github="Dongping-Chen/MLLM-Judge">
                <div class="paper-badge oral">Oral</div>
                <papertitle>MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark</papertitle>
                <div class="paper-authors">Dongping Chen*, Ruoxi Chen*, Shilin Zhang*, Yaochen Wang*, Yinuo Liu*, <strong>Huichi Zhou*</strong>, Qihui Zhang*, Yao Wan, Pan Zhou, Lichao Sun</div>
                <div class="paper-venue">ICML 2024 Oral</div>
                <div class="paper-links">
                  <a href="https://openreview.net/pdf?id=dbFEFHAD79" target="_blank">üìÑ Paper</a>
                  <a href="https://github.com/Dongping-Chen/MLLM-Judge" target="_blank" class="github-link">üíª Code <span class="github-stars" data-repo="Dongping-Chen/MLLM-Judge">‚≠ê --</span></a>
                </div>
              </div>

            </div>
          </div>

        </div>


        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Reviewer Services</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <td width="75%" valign="center">
              ICLR 2025
              <br>
              ACL ARR 2024-2025
              <br>
              <!-- Volunteer: <a href="images/wine2020.pdf">WINE <em>2020</em></a> -->
            </td>
          </tr>
        </tbody></table>


        <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Teaching</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>

          <tr>
            <td width="75%" valign="center">
                Teaching Assistant:
            <br>
              &nbsp &nbsp &nbsp <a href="https://deep-generative-models.github.io/">Deep Generative Models</a>, <em> 2020, 2022 </em>
            <br>
                Guest Lecturer:
            <br>
              &nbsp &nbsp &nbsp Frontier Computing Research Practice (Course of Open Source Development), <em> 2024 </em>
            <br>
              &nbsp &nbsp &nbsp Introduction to Computing (Course of Dynamic Programming), <em> 2024 </em>
            </td>
          </tr>

        </tbody></table>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Invited Talks</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <td width="75%" valign="center">
              Learning 3D Visual Representations for Robotic Manipulation,
              <br>
              &nbsp &nbsp &nbsp <em>National University of Singapore, Feb. 2025</em>
              <br>
              &nbsp &nbsp &nbsp <em>Beijing Normal University, Shanghai Tech University, TeleAI, Jan. 2025</em>
              <br>
              &nbsp &nbsp &nbsp <em>Tsinghua University, Johns Hopkins University, Carnegie Mellon University, Dec. 2024</em>
              <br>
              &nbsp &nbsp &nbsp <em>University of Hong Kong, Nov. 2024</em>
              <br>
              Unified Simulation, Benchmark and Manipulation for Garments, &nbsp&nbsp&nbsp  <em><a href="https://anysyn3d.github.io/about.html">AnySyn3D</a>, 2024</em>
              <br>
                &nbsp &nbsp &nbsp --- If you are interested in 3D Vision research, welcome to follow <a href="https://anysyn3d.github.io/about.html">AnySyn3D</a> that conducts various topics.
              <br>
              Visual Representations for Embodied Agent, &nbsp&nbsp&nbsp  <em>Chinese University of Hong Kong, Shenzhen, 2024</em>
              <br>
              <a href="http://www.csig3dv.net/2024/studentFotum.html">Visual Representations for Embodied Agent</a>, &nbsp&nbsp&nbsp  <em>China3DV, 2024</em>
              <br>
            </td>
          </tr>

        </tbody></table> -->


        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Honors and Awards</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <td width="75%" valign="center">
              Chinese National Scholarship (top1%)  <em> 2023</em>
              <br>
              Excellent Graduation Thesis Award (top1%) <em> 2024 </em>
              <br>
            </td>
          </tr>

        </tbody></table>
      </td>
    </tr>
  </table>

  <script>
    // Toggle category collapse/expand
    function toggleCategory(categoryId) {
      const content = document.getElementById(categoryId + '-content');
      const toggle = document.getElementById(categoryId + '-toggle');

      content.classList.toggle('collapsed');
      toggle.classList.toggle('rotated');
    }

    // Fetch GitHub stars for individual repositories
    async function fetchRepoStars(repo, element) {
      try {
        const response = await fetch(`https://api.github.com/repos/${repo}`);
        const data = await response.json();
        if (data.stargazers_count !== undefined) {
          element.textContent = `‚≠ê ${data.stargazers_count}`;
        }
      } catch (error) {
        console.error(`Error fetching stars for ${repo}:`, error);
        element.textContent = '‚≠ê --';
      }
    }

    // Fetch all GitHub stars for papers
    async function fetchAllGitHubStars() {
      const starElements = document.querySelectorAll('.github-stars');
      starElements.forEach(element => {
        const repo = element.getAttribute('data-repo');
        if (repo) {
          fetchRepoStars(repo, element);
        }
      });
    }

    // Fetch Google Scholar citations using web scraping
    async function fetchScholarCitations() {
      try {
        // Method 1: Using a CORS proxy (you may need to set up your own)
        const scholarId = '1IJyxpUAAAAJ';
        const proxyUrl = 'https://api.allorigins.win/raw?url=';
        const scholarUrl = `https://scholar.google.com/citations?user=${scholarId}&hl=en`;

        const response = await fetch(proxyUrl + encodeURIComponent(scholarUrl));
        const html = await response.text();

        // Parse the HTML to extract citation count
        const parser = new DOMParser();
        const doc = parser.parseFromString(html, 'text/html');

        // Look for the citation count in the table
        const citationTable = doc.querySelector('#gsc_rsb_st');
        if (citationTable) {
          const rows = citationTable.querySelectorAll('tr');
          if (rows[1]) {
            const cells = rows[1].querySelectorAll('td');
            if (cells[1]) {
              const citations = parseInt(cells[1].textContent.trim());
              if (!isNaN(citations)) {
                const citationsElement = document.getElementById('citation-count-inline');
                if (citationsElement) {
                  citationsElement.textContent = citations;
                }
                return;
              }
            }
          }
        }

        // Fallback: Try to find citation count in meta tags or text
        const allText = doc.body.textContent;
        const citationMatch = allText.match(/Citations\s*(\d+)/i) || allText.match(/Cited by\s*(\d+)/i);
        if (citationMatch && citationMatch[1]) {
          const citations = parseInt(citationMatch[1]);
          const citationsElement = document.getElementById('citation-count-inline');
          if (citationsElement) {
            citationsElement.textContent = citations;
          }
          return;
        }
      } catch (error) {
        console.error('Error fetching Scholar citations:', error);
      }

      // Fallback: Keep default display
      const citationsElement = document.getElementById('citation-count-inline');
      if (citationsElement && citationsElement.textContent === 'Loading...') {
        citationsElement.textContent = '150+';
      }
    }

    // Alternative method: Using Serpapi (requires API key)
    async function fetchScholarCitationsViaSerpapi() {
      // You can sign up for a free API key at https://serpapi.com/
      const apiKey = 'YOUR_SERPAPI_KEY'; // Replace with your API key
      const scholarId = '1IJyxpUAAAAJ';

      if (apiKey === 'YOUR_SERPAPI_KEY') {
        // If no API key, use fallback
        fetchScholarCitations();
        return;
      }

      try {
        const response = await fetch(`https://serpapi.com/search.json?engine=google_scholar_author&author_id=${scholarId}&api_key=${apiKey}`);
        const data = await response.json();

        if (data.cited_by && data.cited_by.table && data.cited_by.table[0]) {
          const citations = data.cited_by.table[0].citations.all;
          const citationsElement = document.getElementById('citation-count');
          animateNumber(citationsElement, citations);
        }
      } catch (error) {
        console.error('Error fetching Scholar citations via Serpapi:', error);
        fetchScholarCitations();
      }
    }

    // Animate number counting
    function animateNumber(element, target) {
      const duration = 2000;
      const start = 0;
      const increment = target / (duration / 16);
      let current = start;

      const timer = setInterval(() => {
        current += increment;
        if (current >= target) {
          element.textContent = target.toLocaleString();
          clearInterval(timer);
        } else {
          element.textContent = Math.floor(current).toLocaleString();
        }
      }, 16);
    }

    // Smooth scroll reveal animations
    document.addEventListener('DOMContentLoaded', function() {
      // Fetch stats
      fetchAllGitHubStars();
      fetchScholarCitations();

      // Add animation to category sections
      const categories = document.querySelectorAll('.category-section');
      categories.forEach((section, index) => {
        section.style.opacity = '0';
        section.style.transform = 'translateY(20px)';
        setTimeout(() => {
          section.style.transition = 'all 0.6s ease-out';
          section.style.opacity = '1';
          section.style.transform = 'translateY(0)';
        }, 150 * index);
      });

      // Add animation classes to publication items
      const publicationRows = document.querySelectorAll('.paper-card');
      publicationRows.forEach((row, index) => {
        row.style.opacity = '0';
        row.style.transform = 'translateY(20px)';
        row.style.transition = 'all 0.6s ease-out';

        setTimeout(() => {
          row.style.opacity = '1';
          row.style.transform = 'translateY(0)';
        }, 50 * index);
      });

      // Add parallax effect to background
      document.addEventListener('mousemove', (e) => {
        const x = (e.clientX / window.innerWidth) - 0.5;
        const y = (e.clientY / window.innerHeight) - 0.5;

        document.body.style.backgroundPosition = `${50 + x * 10}% ${50 + y * 10}%`;
      });

      // Intersection Observer for scroll animations
      const observerOptions = {
        threshold: 0.1,
        rootMargin: '0px 0px -100px 0px'
      };

      const observer = new IntersectionObserver((entries) => {
        entries.forEach(entry => {
          if (entry.isIntersecting) {
            entry.target.classList.add('active');
          }
        });
      }, observerOptions);

      // Add hover effect to profile image
      const profileImg = document.querySelector('img[alt="profile photo"]');
      if (profileImg) {
        profileImg.style.cursor = 'pointer';
      }

      // Add number counter animation for headings
      const headings = document.querySelectorAll('heading');
      headings.forEach(heading => {
        heading.style.opacity = '0';
        heading.style.transform = 'translateX(-20px)';
        heading.style.transition = 'all 0.5s ease-out';

        setTimeout(() => {
          heading.style.opacity = '1';
          heading.style.transform = 'translateX(0)';
        }, 200);
      });

      // Create floating particles effect
      createFloatingParticles();
    });

    // Create subtle floating particles
    function createFloatingParticles() {
      const particleCount = 15;
      const body = document.body;

      for (let i = 0; i < particleCount; i++) {
        const particle = document.createElement('div');
        particle.className = 'particle';
        particle.style.cssText = `
          position: fixed;
          width: ${Math.random() * 4 + 2}px;
          height: ${Math.random() * 4 + 2}px;
          background: rgba(255, 255, 255, 0.5);
          border-radius: 50%;
          pointer-events: none;
          z-index: 0;
          left: ${Math.random() * 100}%;
          top: ${Math.random() * 100}%;
          animation: float-particle ${Math.random() * 10 + 10}s infinite ease-in-out;
          animation-delay: ${Math.random() * 5}s;
        `;
        body.appendChild(particle);
      }

      // Add keyframes for particle animation
      if (!document.getElementById('particle-animation')) {
        const style = document.createElement('style');
        style.id = 'particle-animation';
        style.textContent = `
          @keyframes float-particle {
            0%, 100% {
              transform: translate(0, 0) scale(1);
              opacity: 0;
            }
            10% {
              opacity: 1;
            }
            90% {
              opacity: 1;
            }
            50% {
              transform: translate(${Math.random() * 100 - 50}px, ${Math.random() * 100 - 50}px) scale(1.5);
            }
          }
        `;
        document.head.appendChild(style);
      }
    }

    // Add smooth scroll for anchor links
    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
      anchor.addEventListener('click', function (e) {
        e.preventDefault();
        const target = document.querySelector(this.getAttribute('href'));
        if (target) {
          target.scrollIntoView({
            behavior: 'smooth',
            block: 'start'
          });
        }
      });
    });

    // Add loading animation
    window.addEventListener('load', () => {
      document.body.style.opacity = '0';
      setTimeout(() => {
        document.body.style.transition = 'opacity 0.5s ease-in';
        document.body.style.opacity = '1';
      }, 100);
    });
  </script>
</body>

</html>
