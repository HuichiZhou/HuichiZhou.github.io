<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Huichi Zhou (周辉池)</title>
  
  <meta name="author" content="Huichi Zhou">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>⭐️</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Huichi Zhou</name>
              </p>
              <p>
                Huichi Zhou is currently a graduate student at <a href="https://www.imperial.ac.uk/">Imperial College London</a> with 
                YangLab advised by Professor <a href="https://scholar.google.com.mx/citations?hl=zh-CN&user=ZfzEFpsAAAAJ&view_op=list_works">Guang Yang</a>. 
		Currently, I collaborate with Professor <a href="https://scholar.google.com/citations?hl=en&user=wIE1tY4AAAAJ&view_op=list_works">Jun Wang</a> and Dr. <a href="https://scholar.google.com/citations?user=go3sFxcAAAAJ#">Linyi Yang</a>. 
              </p>
              <p>
                  My research interest is broadly in Adversarial Machine Learning and Large Language Models.
	      </p>
              <p>
                Email: h.zhou24 [at] imperial.ac.uk 
              </p>
	      <p>
                <a href="https://github.com/HuichiZhou">Github</a> &nbsp &nbsp <a href="https://scholar.google.com/citations?user=1IJyxpUAAAAJ&hl=en">Google Scholar</a>
              </p>


            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/zhc.jpg"><img style="width:80%;max-width:100%" alt="profile photo" src="images/zhc.jpg" class="hoverZoomLink"></a>
            </td>
		  
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Selected Publications</heading>&nbsp&nbsp&nbsp&nbsp&nbsp (* denotes equal contribution)
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:30px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
<!-- Publications List without Images -->
<table style="width:100%; border-collapse:collapse;">

  <!-- DiffuseDef -->
  <tr>
    <td style="padding:20px; vertical-align:middle;">
      <papertitle>DiffuseDef: Improve dRobustness to Adversarial Attacks via Iterative Denoising</papertitle><br>
      Zhenhao Li, <strong>Huichi Zhou</strong>, Marek Rei, Lucia Specia<br>
      <em>ACL 2025</em><br>
      <a href="https://arxiv.org/abs/2407.00248">paper</a>
      <p>We propose DiffuseDef, an adversarial defense method for language classification tasks. The method inserts a diffusion layer between the encoder and classifier to iteratively denoise adversarially perturbed hidden states. It is evaluated on datasets like AG News, IMDB, and QNLI against both black-box and white-box attacks.</p>
    </td>
  </tr>

  <!-- Beyond the Hype -->
  <tr>
    <td style="padding:20px; vertical-align:middle;">
      <papertitle>Beyond the Hype: A Dispassionate Look at Vision-Language Models in Medical Scenario</papertitle><br>
      Yang Nan*, <strong>Huichi Zhou*</strong>, Xiaodan Xing, Guang Yang<br>
      <em>IEEE Transactions on Neural Networks and Learning Systems (TNNLS), Impact Factor 10.2</em><br>
      <a href="https://arxiv.org/abs/2408.08704">paper</a>
      <p>We introduce RadVUQA, a novel Radiological Visual Understanding and Question Answering benchmark, to comprehensively evaluate existing LVLMs.</p>
    </td>
  </tr>

  <!-- Revisiting Medical Image Retrieval -->
  <tr>
    <td style="padding:20px; vertical-align:middle;">
      <papertitle>Revisiting Medical Image Retrieval via Knowledge Consolidation</papertitle><br>
      Yang Nan, <strong>Huichi Zhou</strong>, Xiaodan Xing, Giorgos Papanastasiou, Lei Zhu, Zhifan Gao, Alejandro F. Frangi, Guang Yang<br>
      <em>Medical Image Analysis, Impact Factor 10.7</em><br>
      <a href="https://arxiv.org/pdf/2503.09370">paper</a>
      <p>ACIR (Anomaly-aware Content-based Image Recommendation) is a medical image retrieval framework that enhances accuracy and robustness through multi-level feature fusion (DaRF), structure-aware contrastive hashing (SCH), self-supervised OOD detection, and content-guided ranking. It achieves up to 38.9% improvement in mAP on anatomical datasets over existing methods.</p>
    </td>
  </tr>

  <!-- TrustRAG -->
  <tr>
    <td style="padding:20px; vertical-align:middle;">
      <papertitle>TrustRAG: Enhancing Robustness and Trustworthiness in RAG</papertitle><br>
      <strong>Huichi Zhou*</strong>, Kin-Hei Lee*, Zhonghao Zhan*, Yue Chen, Zhenhao Li, Zhaoyang Wang, Hamed Haddadi, Emine Yilmaz<br>
      <em>Under Review</em><br>
      <a href="https://trust-rag.github.io/">project page</a> / <a href="https://arxiv.org/pdf/2501.00879">paper</a> / <a href="https://github.com/HuichiZhou/TrustRAG">code</a>
      <p>We introduce TrustRAG, a robust Retrieval-Augmented Generation (RAG) framework. It defends against corpus poisoning attacks by a two-stage mechanism: identifying potential attack patterns with K-means clustering and detecting malicious documents via self-assessment.</p>
    </td>
  </tr>

  <!-- Verifiable Format Control -->
  <tr>
    <td style="padding:20px; vertical-align:middle;">
      <papertitle>Verifiable Format Control for Large Language Model Generations</papertitle><br>
      Zhaoyang Wang*, Jinqi Jiang*, <strong>Huichi Zhou*</strong>, Wenhao Zheng, Xuchao Zhang, Chetan Bansal, Huaxiu Yao<br>
      <em>NAACL 2025 Findings</em><br>
      <a href="https://arxiv.org/pdf/2502.04498">paper</a> / <a href="https://huggingface.co/datasets/jinqij/VFF">code</a>
      <p>We introduce VFF, a dataset and training framework that improves the format-following ability of 7B-level LLMs using Python-based verification and progressive training. It enhances LLMs’ ability to follow specific formats (e.g., JSON) through self-generated data and direct preference optimization (DPO).</p>
    </td>
  </tr>

  <!-- LLM4RGNN -->
  <tr>
    <td style="padding:20px; vertical-align:middle;">
      <papertitle>Can Large Language Models Improve the Adversarial Robustness of Graph Neural Networks?</papertitle><br>
      Zhongjian Zhang*, Xiao Wang*, <strong>Huichi Zhou</strong>, Mengmei Zhang, Cheng Yang, Chuan Shi<br>
      <em>KDD 2025 Research Track</em><br>
      <a href="https://arxiv.org/pdf/2408.08685">paper</a> / <a href="https://github.com/zhongjian-zhang/LLM4RGNN">code</a>
      <p>LLM4RGNN is a framework that enhances the adversarial robustness of Graph Neural Networks (GNNs) using Large Language Models (LLMs). By removing malicious edges and adding important ones identified via distilled GPT-4 inference, it significantly improves GNN performance under adversarial attacks.</p>
    </td>
  </tr>

  <!-- AVLLM -->
  <tr>
    <td style="padding:20px; vertical-align:middle;">
      <papertitle>Evaluating the Validity of Word-level Adversarial Attacks with Large Language Models</papertitle><br>
      <strong>Huichi Zhou*</strong>, Zhaoyang Wang*, Hongtao Wang, Dongping Chen, Wenhan Mu, Fangyuan Zhang<br>
      <em>ACL 2024 Findings</em><br>
      <a href="https://aclanthology.org/2024.findings-acl.292">paper</a> / <a href="https://github.com/HuichiZhou/AVLLM">code</a>
      <p>AVLLM is a framework for evaluating and improving the validity of word-level adversarial attacks on LLMs. It fine-tunes a lightweight LLM to provide a validity score and explanation, helping to generate semantically consistent adversarial examples.</p>
    </td>
  </tr>

  <!-- GUI-World -->
  <tr>
    <td style="padding:20px; vertical-align:middle;">
      <papertitle>GUI-World: A Video Benchmark and Dataset for Multimodal GUI-oriented Understanding</papertitle><br>
      Dongping Chen, Yue Huang, Siyuan Wu, Jingyu Tang, <strong>Huichi Zhou</strong>, Qihui Zhang, Zhigang He, Yilin Bai, Chujie Gao, Liuyi Chen, Yiqiang Li, Chenlong Wang, Yue Yu, Tianshuo Zhou, Zhen Li, Yi Gui, Yao Wan, Pan Zhou, Jianfeng Gao, Lichao Sun<br>
      <em>ICLR 2025</em><br>
      <a href="https://openreview.net/forum?id=QarKTT5brZ">paper</a> / <a href="https://github.com/Dongping-Chen/GUI-World">code</a>
      <p>GUI-WORLD is a comprehensive video-based benchmark and dataset designed for GUI-oriented multimodal understanding. It includes 12,379 GUI videos across six scenarios and eight question types, addressing challenges faced by existing models.</p>
    </td>
  </tr>

  <!-- MLLM-as-a-Judge -->
  <tr>
    <td style="padding:20px; vertical-align:middle;">
      <papertitle>MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark</papertitle><br>
      Dongping Chen*, Ruoxi Chen*, Shilin Zhang*, Yaochen Wang*, Yinuo Liu*, <strong>Huichi Zhou*</strong>, Qihui Zhang*, Yao Wan, Pan Zhou, Lichao Sun<br>
      <em>ICML 2024 Oral</em><br>
      <a href="https://openreview.net/pdf?id=dbFEFHAD79">paper</a> / <a href="https://github.com/Dongping-Chen/MLLM-Judge">code</a>
      <p>MLLM-as-a-Judge is a benchmark designed to evaluate the judging capabilities of Multimodal Large Language Models (MLLMs) in vision-language tasks. It assesses MLLMs on scoring, pair comparison, and batch ranking across 14 datasets, highlighting consistency and reasoning challenges.</p>
    </td>
  </tr>

</table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Reviewer Services</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <td width="75%" valign="center">
              ICLR 2025
              <br>
              ACL ARR 2024-2025
              <br>
              <!-- Volunteer: <a href="images/wine2020.pdf">WINE <em>2020</em></a> -->
            </td>
          </tr>
        </tbody></table>


        <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Teaching</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>

          <tr>
            <td width="75%" valign="center">
                Teaching Assistant:
            <br>
              &nbsp &nbsp &nbsp <a href="https://deep-generative-models.github.io/">Deep Generative Models</a>, <em> 2020, 2022 </em>
            <br>
                Guest Lecturer:
            <br>
              &nbsp &nbsp &nbsp Frontier Computing Research Practice (Course of Open Source Development), <em> 2024 </em>
            <br>
              &nbsp &nbsp &nbsp Introduction to Computing (Course of Dynamic Programming), <em> 2024 </em>
            </td>
          </tr>

        </tbody></table>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Invited Talks</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <td width="75%" valign="center">
              Learning 3D Visual Representations for Robotic Manipulation,
              <br>
              &nbsp &nbsp &nbsp <em>National University of Singapore, Feb. 2025</em>
              <br>
              &nbsp &nbsp &nbsp <em>Beijing Normal University, Shanghai Tech University, TeleAI, Jan. 2025</em>
              <br>
              &nbsp &nbsp &nbsp <em>Tsinghua University, Johns Hopkins University, Carnegie Mellon University, Dec. 2024</em>
              <br>
              &nbsp &nbsp &nbsp <em>University of Hong Kong, Nov. 2024</em>
              <br>
              Unified Simulation, Benchmark and Manipulation for Garments, &nbsp&nbsp&nbsp  <em><a href="https://anysyn3d.github.io/about.html">AnySyn3D</a>, 2024</em>
              <br>
                &nbsp &nbsp &nbsp --- If you are interested in 3D Vision research, welcome to follow <a href="https://anysyn3d.github.io/about.html">AnySyn3D</a> that conducts various topics.
              <br>
              Visual Representations for Embodied Agent, &nbsp&nbsp&nbsp  <em>Chinese University of Hong Kong, Shenzhen, 2024</em>
              <br>
              <a href="http://www.csig3dv.net/2024/studentFotum.html">Visual Representations for Embodied Agent</a>, &nbsp&nbsp&nbsp  <em>China3DV, 2024</em>
              <br>
            </td>
          </tr>

        </tbody></table> -->


        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Honors and Awards</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <td width="75%" valign="center">
              Chinese National Scholarship (top1%)  <em> 2023</em>
              <br>
              Excellent Graduation Thesis Award (top1%) <em> 2024 </em>
              <br>
            </td>
          </tr>

        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
                  <div style="float:right;">
                    Website template comes from <a href="https://jonbarron.info/">Jon Barron</a><br>
                      Last update: March, 2025
                  </div>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
